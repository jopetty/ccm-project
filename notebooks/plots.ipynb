{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyrootutils\n",
    "import seaborn as sns\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from src.tokenizer_metrics import (\n",
    "    AlignmentWithCDI,\n",
    "    AverageTokenLength,\n",
    "    CorrespondenceWithMorphemes,\n",
    "    CorrespondenceWithWords,\n",
    "    SingleTokenizerMetric,\n",
    "    SplitsIntoMorphemes,\n",
    "    SplitsOnSpace,\n",
    "    TokenizerOverlap,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = path = pyrootutils.find_root(\n",
    "    search_from=os.path.abspath(\"\"), indicator=\".project-root\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizers_from_dir(dir: Path, name: str = None):\n",
    "    partial_path = PROJECT_ROOT / dir\n",
    "    glob_path = partial_path / \"[0-9]*-tokenizer.json\"\n",
    "    n_tokenizers = len(glob.glob(str(glob_path)))\n",
    "\n",
    "    tokenizers = []\n",
    "    for i in range(n_tokenizers):\n",
    "        tokenizer_base = Tokenizer.from_file(str(partial_path / f\"{i}-tokenizer.json\"))\n",
    "        tokenizer_base.name = f\"{name}-{i}\"\n",
    "        tokenizers.append(tokenizer_base)\n",
    "\n",
    "    return tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = {}\n",
    "\n",
    "tkz_dict = {\n",
    "    \"LM-merge-1\": \"outputs/2024-05-06-125114_93f0\",\n",
    "    \"LM-nomerge-1\": \"outputs/2024-05-06-125115_9621\",\n",
    "    \"LM-merge-2\": \"outputs/2024-05-06-125411_bfc9\",\n",
    "    \"LM-nomerge-2\": \"outputs/2024-05-06-125411_90aa\",\n",
    "    \"LM-merge-3\": \"outputs/2024-05-06-125412_5c63\",\n",
    "    \"LM-nomerge-3\": \"outputs/2024-05-06-125411_10c9\",\n",
    "    \"LM-merge-4\": \"outputs/2024-05-06-125411_e5c5\",\n",
    "    \"LM-nomerge-4\": \"outputs/2024-05-06-125411_e6e7\",\n",
    "    \"LM-merge-5\": \"outputs/2024-05-06-125411_1d73\",\n",
    "    \"BPE-nomerge-retrain\": \"outputs/bpe-tokenizers/2024-05-10-114128_b1a1\",\n",
    "    \"BPE-nomerge-noretrain\": \"outputs/bpe-tokenizers/2024-05-10-115123_88af\",\n",
    "    \"BPE-merge-retrain\": \"outputs/bpe-tokenizers/2024-05-10-120039_8a27\",\n",
    "    \"BPE-merge-noretrain\": \"outputs/bpe-tokenizers/2024-05-10-123454_58e0\",\n",
    "}\n",
    "\n",
    "for name, dir in tkz_dict.items():\n",
    "    tokenizers[name] = get_tokenizers_from_dir(dir, name=name)\n",
    "\n",
    "tkz_info = []\n",
    "for name, ts in tokenizers.items():\n",
    "    tkz_info.append(\n",
    "        {\n",
    "            \"name\": name,\n",
    "            \"n_epochs\": len(ts),\n",
    "            \"final_vocab_size\": ts[-1].get_vocab_size(),\n",
    "            \"merge_across_spaces\": \"nomerge\" not in name,\n",
    "        }\n",
    "    )\n",
    "\n",
    "tkz_info_df = pd.DataFrame(tkz_info)\n",
    "tkz_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkz_info_df.groupby(\"merge_across_spaces\")[[\"final_vocab_size\", \"n_epochs\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multitokenizer_metrics = [AlignmentWithCDI, TokenizerOverlap]\n",
    "metric_names = [\n",
    "    tokenizer_metric.__qualname__ for tokenizer_metric in multitokenizer_metrics\n",
    "]\n",
    "scores = {}\n",
    "for tokenizer_type, tokenizer_list in tokenizers.items():\n",
    "    scores[tokenizer_type] = [\n",
    "        metric(tokenizer_list).calculate() for metric in multitokenizer_metrics\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = []\n",
    "for tknzr in scores.keys():\n",
    "    for mi in range(len(metric_names)):\n",
    "        graph_data.append(\n",
    "            {\n",
    "                \"tokenizer\": tknzr,\n",
    "                \"metric\": metric_names[mi],\n",
    "                \"score\": scores[tknzr][mi],\n",
    "            }\n",
    "        )\n",
    "graph_data_df = pd.DataFrame(graph_data)\n",
    "graph_data_df[\"merge_across_spaces\"] = ~graph_data_df[\"tokenizer\"].str.contains(\n",
    "    \"nomerge\"\n",
    ")\n",
    "\n",
    "graph_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(\n",
    "    data=graph_data_df, kind=\"bar\", x=\"metric\", y=\"score\", hue=\"merge_across_spaces\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(metric: dict[str, float], metric_name: str):\n",
    "    plt.figure()\n",
    "    plt.clf()\n",
    "    for tok_type, values in metric.items():\n",
    "        plt.plot(values, label=tok_type)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Tokenizer Iteration\")\n",
    "    plt.title(metric_name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_and_plot(metric_name: SingleTokenizerMetric, **args):\n",
    "    values = {}\n",
    "    for tok_type, tok_list in tokenizers.items():\n",
    "        scores = []\n",
    "        for tok in tok_list:\n",
    "            x = metric_name(tok, **args)\n",
    "            scores.append(x.calculate())\n",
    "        values[tok_type] = scores\n",
    "    plot_comparison(values, metric_name=metric_name.__qualname__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of spaces that correspond to token boundaries\n",
    "scores = []\n",
    "for tok in tokenizers[\"BPE-noretrain-1000x20-nosplitspace\"]:\n",
    "    x = SplitsOnSpace(tok)\n",
    "    scores.append(x.calculate())\n",
    "plot_comparison({\"no space\": scores}, metric_name=SplitsOnSpace.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of tokenized boundaries that correspond to actual spaces\n",
    "scores = []\n",
    "for tok in tokenizers[\"Nospace\"]:\n",
    "    x = SplitsOnSpace(tok, baseline=\"tokenized\")\n",
    "    scores.append(x.calculate())\n",
    "plot_comparison({\"no space\": scores}, metric_name=SplitsOnSpace.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_and_plot(AverageTokenLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_and_plot(CorrespondenceWithMorphemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_and_plot(CorrespondenceWithWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_and_plot(SplitsIntoMorphemes, metric=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_and_plot(SplitsIntoMorphemes, metric=\"distance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccm_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
