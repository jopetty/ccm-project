{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyrootutils\n",
    "import seaborn as sns\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from src.tokenizer_metrics import (\n",
    "    AlignmentWithCDI,\n",
    "    AverageTokenLength,\n",
    "    CorrespondenceWithMorphemes,\n",
    "    CorrespondenceWithWords,\n",
    "    SingleTokenizerMetric,\n",
    "    SplitsIntoMorphemes,\n",
    "    SplitsOnSpace,\n",
    "    TokenizerOverlap,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = path = pyrootutils.find_root(\n",
    "    search_from=os.path.abspath(\"\"), indicator=\".project-root\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizers_from_dir(dir: Path, name: str = None):\n",
    "    partial_path = PROJECT_ROOT / dir\n",
    "    glob_path = partial_path / \"[0-9]*-tokenizer.json\"\n",
    "    n_tokenizers = len(glob.glob(str(glob_path)))\n",
    "\n",
    "    tokenizers = []\n",
    "    for i in range(n_tokenizers):\n",
    "        tokenizer_base = Tokenizer.from_file(str(partial_path / f\"{i}-tokenizer.json\"))\n",
    "        tokenizer_base.name = f\"{name}-{i}\"\n",
    "        tokenizers.append(tokenizer_base)\n",
    "\n",
    "    return tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = {}\n",
    "\n",
    "tokenizers[\"LM-10000x20\"] = get_tokenizers_from_dir(\n",
    "    \"outputs/2024-05-04-221644_5b2e\",\n",
    "    name=\"LM-10000x20\",\n",
    ")\n",
    "\n",
    "tokenizers[\"LM-10000x20-nosplitting\"] = get_tokenizers_from_dir(\n",
    "    \"outputs/2024-05-04-233927_9728\",\n",
    "    name=\"LM-10000x20-nosplitting\",\n",
    ")\n",
    "\n",
    "tokenizers[\"LM-10000x20-splitting\"] = get_tokenizers_from_dir(\n",
    "    \"outputs/2024-05-05-005805_dcb3\",\n",
    "    name=\"LM-10000x20-splitting\",\n",
    ")\n",
    "\n",
    "tokenizers[\"BPE-retrain-10000x20\"] = get_tokenizers_from_dir(\n",
    "    \"outputs/bpe-tokenizers/2024-05-04-232043_a9cd\",\n",
    "    name=\"BPE-retrain-10000x20\",\n",
    ")\n",
    "\n",
    "tokenizers[\"BPE-noretrain-10000x20\"] = get_tokenizers_from_dir(\n",
    "    \"outputs/bpe-tokenizers/2024-05-04-232345_4c17\",\n",
    "    name=\"BPE-noretrain-10000x20\",\n",
    ")\n",
    "\n",
    "tokenizers[\"BPE-noretrain-10000x20-nosplitting\"] = get_tokenizers_from_dir(\n",
    "    \"outputs/bpe-tokenizers/2024-05-04-232529_dd8e\",\n",
    "    name=\"BPE-noretrain-10000x20-nosplitting\",\n",
    ")\n",
    "\n",
    "# tokenizers[\"BPE 10 splits no retrain\"] = get_tokenizers_from_dir(\n",
    "#     # \"outputs/bpe-tokenizers/2024-05-04-121353_5821\", name=\"BPE 10 splits no retrain\"\n",
    "#     # \"outputs/bpe-tokenizers/2024-05-04-140418_3016\", name=\"BPE 10 splits no retrain\"\n",
    "#     \"outputs/bpe-tokenizers/2024-05-04-144627_4d11\",\n",
    "#     name=\"BPE 10 splits no retrain\",\n",
    "# )\n",
    "\n",
    "# tokenizers[\"BPE 10 splits retrain\"] = get_tokenizers_from_dir(\n",
    "#     # \"outputs/bpe-tokenizers/2024-05-04-121534_3769\", name=\"BPE 10 splits retrain\"\n",
    "#     # \"outputs/bpe-tokenizers/2024-05-04-140213_96d4\", name=\"BPE 10 splits retrain\"\n",
    "#     \"outputs/bpe-tokenizers/2024-05-04-144527_93b6\",\n",
    "#     name=\"BPE 10 splits retrain\",\n",
    "# )\n",
    "\n",
    "# tokenizers[\"Nospace\"] = get_tokenizers_from_dir(\n",
    "#     # \"outputs/bpe-tokenizers/2024-05-04-091551_b89e\", suffix=True\n",
    "#     # \"outputs/bpe-tokenizers/2024-05-04-142555_cc5e\", name=\"Nospace\"\n",
    "#     \"outputs/bpe-tokenizers/2024-05-04-144603_489b\",\n",
    "#     name=\"Nospace\",\n",
    "# )\n",
    "\n",
    "# tokenizers[\"AGGHGHG\"] = get_tokenizers_from_dir(\n",
    "#     # \"outputs/2024-05-04-115111_0a5e\", name=\"AGGHGHG\"\n",
    "#     \"outputs/2024-05-04-122039_2270\",\n",
    "#     name=\"AGGHGHG\",\n",
    "# )\n",
    "\n",
    "print(\"Vocab Sizes:\")\n",
    "for name, ts in tokenizers.items():\n",
    "    print(name, len(ts), [t.get_vocab_size() for t in ts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multitokenizer_metrics = [AlignmentWithCDI, TokenizerOverlap]\n",
    "metric_names = [\n",
    "    tokenizer_metric.__qualname__ for tokenizer_metric in multitokenizer_metrics\n",
    "]\n",
    "scores = {}\n",
    "for tokenizer_type, tokenizer_list in tokenizers.items():\n",
    "    scores[tokenizer_type] = [\n",
    "        metric(tokenizer_list).calculate() for metric in multitokenizer_metrics\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = []\n",
    "for tknzr in scores.keys():\n",
    "    for mi in range(len(metric_names)):\n",
    "        graph_data.append(\n",
    "            {\n",
    "                \"tokenizer\": tknzr,\n",
    "                \"metric\": metric_names[mi],\n",
    "                \"score\": scores[tknzr][mi],\n",
    "            }\n",
    "        )\n",
    "graph_data_df = pd.DataFrame(graph_data)\n",
    "graph_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=graph_data_df, kind=\"bar\", x=\"metric\", y=\"score\", hue=\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(metric: dict[str, float], metric_name: str):\n",
    "    plt.figure()\n",
    "    plt.clf()\n",
    "    for tok_type, values in metric.items():\n",
    "        plt.plot(values, label=tok_type)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Tokenizer Iteration\")\n",
    "    plt.title(metric_name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_and_plot(metric_name: SingleTokenizerMetric, **args):\n",
    "    values = {}\n",
    "    for tok_type, tok_list in tokenizers.items():\n",
    "        scores = []\n",
    "        for tok in tok_list:\n",
    "            x = metric_name(tok, **args)\n",
    "            scores.append(x.calculate())\n",
    "        values[tok_type] = scores\n",
    "    plot_comparison(values, metric_name=metric_name.__qualname__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of spaces that correspond to token boundaries\n",
    "scores = []\n",
    "for tok in tokenizers[\"BPE-noretrain-1000x20-nosplitspace\"]:\n",
    "    x = SplitsOnSpace(tok)\n",
    "    scores.append(x.calculate())\n",
    "plot_comparison({\"no space\": scores}, metric_name=SplitsOnSpace.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of tokenized boundaries that correspond to actual spaces\n",
    "scores = []\n",
    "for tok in tokenizers[\"Nospace\"]:\n",
    "    x = SplitsOnSpace(tok, baseline=\"tokenized\")\n",
    "    scores.append(x.calculate())\n",
    "plot_comparison({\"no space\": scores}, metric_name=SplitsOnSpace.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_and_plot(AverageTokenLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_and_plot(CorrespondenceWithMorphemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_and_plot(CorrespondenceWithWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_and_plot(SplitsIntoMorphemes, metric=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_and_plot(SplitsIntoMorphemes, metric=\"distance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccm_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
